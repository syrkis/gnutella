{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysis of the Gnutella Network\n",
    "\n",
    "{INSERT GEPHI PICTURE HERE}\n",
    "\n",
    "Gnutella was a p2p file sharing network from the early 2000's.\n",
    "Nodes were computers (often consumer desktops and laptops)\n",
    "and edges where connections between these.\n",
    "\n",
    "The following presents an analysis of the network as 9 different points in time.\n",
    "Due to the size of the data set, we have divided our anlysis into two parts:\n",
    "1. Computation and saving of various network properties into .json files.\n",
    "2. Plotting and analysing network properties from the .json files.\n",
    "\n",
    "The intention of our analyis is to answer the following:\n",
    "<i>How robust is the Gnutella network?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Table of Contents\n",
    "- Imports\n",
    "- Reading the data\n",
    "- Data Schema\n",
    "- Constructing the json files\n",
    "    - Centrality\n",
    "    - Attack\n",
    "    - Metrics\n",
    "    - Execution\n",
    "- Analysis\n",
    "    - Degree Distribution\n",
    "    - Centraliy Measures\n",
    "        - Closeness\n",
    "        - Betweenness\n",
    "    - Fitting\n",
    "    - Robustness\n",
    "- Conclusion\n",
    "- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## imports\n",
    "Throughout this analysis we use the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import networkx as nx\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import powerlaw\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reading the Data\n",
    "\n",
    "We construct a dictionary `S`  of the 9 networkx objects for use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '../data/graphs/'\n",
    "targets = [path + target for target in os.listdir(path)]\n",
    "S = {idx: nx.read_edgelist(target, delimiter=\"\\t\", create_using=nx.DiGraph(name='test')) \\\n",
    "     for idx, target in enumerate(targets)}\n",
    "for idx, G in enumerate(S):\n",
    "    S[G].name = targets[idx][-6:-4] + '-08-2002'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Scheme\n",
    "\n",
    "Each generated .json file has the following structure:\n",
    "- Original graph networkx output\n",
    "- Configuration model networkx outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Constructing the .json Files\n",
    "\n",
    "The construction of the data files was done by the use of four classes:\n",
    "1. Centrality: Computes centrality measure for a given graph\n",
    "2. Attack: Computes the effect of removing nodes for a given graph\n",
    "3. Metrics: Computes various other graph properties and creates the .json file\n",
    "\n",
    "The classes and the code execution for generating the .json is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Centrality\n",
    "\n",
    "The centrality to class takes in a graph and outputs a dictionary of arrays of\n",
    "various centrality measures. These measures are:\n",
    "\n",
    "- Betweenness centrality\n",
    "- Wigenvector centrality\n",
    "- Closeness centrality\n",
    "- Indegree centraility\n",
    "- Outdegree centrality\n",
    "\n",
    "The code is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Centrality(object):\n",
    "\n",
    "    def __init__(self, G):\n",
    "        self.G = G\n",
    "        self.data = {}\n",
    "\n",
    "    def extract(self):\n",
    "        self.__setup(self.G)\n",
    "        return self.data\n",
    "\n",
    "    def __setup(self, G):\n",
    "        if G.is_multigraph():\n",
    "            G = nx.DiGraph(G)\n",
    "        self.data['betweenness'] = self.__centrality(nx.betweenness_centrality(G, k=1000))\n",
    "        self.data['eigenvector'] = self.__centrality(nx.eigenvector_centrality(G, max_iter=200))\n",
    "        self.data['in_degree'] = self.__centrality(nx.in_degree_centrality(G))\n",
    "        self.data['out_degree'] = self.__centrality(nx.out_degree_centrality(G))\n",
    "        self.data['closeness'] = self.__closeness(G)\n",
    "\n",
    "    def __centrality(self, data):\n",
    "        w = np.ones_like(list(data.values())) / (len(data.values()))\n",
    "        n, x, _ = plt.hist(list(data.values()), bins=20, weights=w)\n",
    "        plt.close()\n",
    "        bin_centers = 0.5*(x[1:]+x[:-1])\n",
    "        return list(bin_centers), list(n)\n",
    "\n",
    "    def __closeness(self, G):\n",
    "        data = {}\n",
    "        L = self.__component(G)\n",
    "        for node in random.sample(L.nodes(), 1000):\n",
    "            data[node] = nx.closeness_centrality(L, u=node)\n",
    "        x, y = self.__centrality(data)\n",
    "        return list(x), list(y), data\n",
    "\n",
    "    def __component(self, G):\n",
    "        components = sorted(nx.strongly_connected_components(G), key=len)[::-1]\n",
    "        L = G.subgraph(components[0])\n",
    "        return L\n",
    "\n",
    "def centrality(G):\n",
    "    C = Centrality(G).extract()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Robustness\n",
    "\n",
    "Central to our research question,\n",
    "the robust class is used to test how the network responds to\n",
    "removing nodes in the following ways:\n",
    "\n",
    "- Randomly\n",
    "- By degree\n",
    "- By eigenvector centrality\n",
    "- By some other bull shit.\n",
    "\n",
    "We considered removing edges, however, it perhaps does not make much conceptual sense\n",
    "for our network, as a computer is either online or not, and not partially online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Attack(object):\n",
    "\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.L = self.__component(self.graph)\n",
    "\n",
    "    def random(self, p):\n",
    "        G = self.L\n",
    "        nodes = random.sample(G.nodes, int(float(len(G.nodes)) * (1-p)))\n",
    "        S = nx.subgraph(G, nodes)\n",
    "        return S\n",
    "\n",
    "    def degrees(self, p):\n",
    "        G = self.L\n",
    "        degs = sorted(G.degree, key=itemgetter(1))[::-1]\n",
    "        nodes = [entry[0] for entry in degs][int(len(degs) * p):]\n",
    "        S = nx.subgraph(G, nodes)\n",
    "        return S\n",
    "\n",
    "    def eigen(self, p):\n",
    "        G = self.L\n",
    "        if G.is_multigraph():\n",
    "            G = nx.DiGraph(G)\n",
    "        eiges = nx.eigenvector_centrality(G, max_iter=200)\n",
    "        nodes = [(k, v) for k, v in eiges.items()]\n",
    "        nodes = sorted(nodes, key=itemgetter(1))[::-1]\n",
    "        nodes = [entry[0] for entry in nodes][int(len(nodes) * p):]\n",
    "        S = nx.subgraph(G, nodes)\n",
    "        return S\n",
    "\n",
    "    def pagerank(self, p):\n",
    "        G = self.L\n",
    "        if G.is_multigraph():\n",
    "            G = nx.DiGraph(G)\n",
    "        eiges = nx.pagerank(G)\n",
    "        nodes = [(k, v) for k, v in eiges.items()]\n",
    "        nodes = sorted(nodes, key=itemgetter(1))[::-1]\n",
    "        nodes = [entry[0] for entry in nodes][int(len(nodes) * p):]\n",
    "        S = nx.subgraph(G, nodes)\n",
    "        return S\n",
    "\n",
    "    def __component(self, G):\n",
    "        components = sorted(nx.strongly_connected_components(G), key=len)[::-1]\n",
    "        L = G.subgraph(components[0])\n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Metrics\n",
    "\n",
    "Putting together the results of running the previous two classes on each og our 9\n",
    "graphs and their corresponding configuration models, while also adding various other\n",
    "data like degree distributions, etc. In other words, this class constructs the .json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Metric:\n",
    "\n",
    "    def __init__(self, G, data={}):\n",
    "        self.G = G\n",
    "        self.U = nx.Graph(G)\n",
    "        self.name = G.name\n",
    "        self.data = data\n",
    "\n",
    "    def build(self):\n",
    "        self.__setup(self.G)\n",
    "\n",
    "    def save(self):\n",
    "        json_data = json.dumps(self.data, indent=4)\n",
    "        with open(f'../data/dumps/{self.name}.json', 'w') as json_file:\n",
    "            json_file.write(json_data)\n",
    "\n",
    "    def __setup(self, G):\n",
    "        G_degs = self.__degfreq(G)         # degree distribution\n",
    "        self.data['degs'] = {'in': G_degs[0], 'out': G_degs[1],\n",
    "                             'ccdf_in': G_degs[2], 'ccdf_out': G_degs[3]}\n",
    "        self.data['centrality'] = centrality(G)         # centrality measures, clustering, knn\n",
    "        self.data['clustering'] = self.__clustering(G)\n",
    "        knn = nx.k_nearest_neighbors(G)\n",
    "        self.data['knn'] = [list(knn.keys()), list(knn.values())]\n",
    "        self.data['attack'] = self.__attack(G)         # attacks\n",
    "        C = self.__config(G)         # configuration metrics\n",
    "        knn = nx.k_nearest_neighbors(C)\n",
    "        self.data['C'] = {'attack': self.__attack(C), 'centrality': centrality(C),\n",
    "                          'clustering': self.__clustering(C),\n",
    "                          'knn': [list(knn.keys()), list(knn.values())]}\n",
    "\n",
    "    def __attack(self, G):\n",
    "        A = Attack(G)\n",
    "        ps = [0.05 * i for i in range(0, 20)]\n",
    "        rs, ds, es, pr = [], [], [], []\n",
    "        for p in ps:\n",
    "            r = A.random(p)\n",
    "            d = A.degrees(p)\n",
    "            e = A.eigen(p)\n",
    "            pa = A.pagerank(p)\n",
    "            rs.append(self.__connectivity(r))\n",
    "            ds.append(self.__connectivity(d))\n",
    "            es.append(self.__connectivity(e))\n",
    "            pr.append(self.__connectivity(pa))\n",
    "        return rs, ds, es, pr, ps\n",
    "\n",
    "    def __degfreq(self, G):\n",
    "        in_freq = Counter(dict(G.in_degree).values())\n",
    "        in_x = list(in_freq.keys())\n",
    "        in_y = list(in_freq.values())\n",
    "        out_freq = Counter(dict(G.out_degree).values())\n",
    "        out_x = list(out_freq.keys())\n",
    "        out_y = list(out_freq.values())\n",
    "        ins = list(dict(G.in_degree).values())      # ccdf setup\n",
    "        outs = list(dict(G.out_degree).values())    # ccdf setup\n",
    "        in_ccdf_x, in_ccdf_y = self.__ccdf(ins)\n",
    "        out_ccdf_x, out_ccdf_y = self.__ccdf(outs)\n",
    "        return [in_x, in_y], [out_x, out_y], [in_ccdf_x, in_ccdf_y], [out_ccdf_x, out_ccdf_y]\n",
    "\n",
    "    def __ccdf(self, degs):\n",
    "        n, x, _ = plt.hist(degs, density=True, cumulative=True, bins=100); plt.close()\n",
    "        x = 0.5 * (x[1:] + x[:-1])\n",
    "        y = [1 - v for v in n]\n",
    "        return [list(x), list(y)]\n",
    "\n",
    "    def __clustering(self, G):\n",
    "        if G.is_multigraph():\n",
    "            G = nx.DiGraph(G)\n",
    "        cluster = Counter(dict(nx.clustering(G)).values())\n",
    "        x = list(cluster.keys())\n",
    "        y = list(cluster.values())\n",
    "        return x, y\n",
    "\n",
    "    def __config(self, G):\n",
    "        d_in = [G.in_degree[node] for node in G.nodes]\n",
    "        d_out = [G.out_degree[node] for node in G.nodes]\n",
    "        return nx.directed_configuration_model(d_in, d_out)\n",
    "\n",
    "    def __connectivity(self, G):\n",
    "        L = self.__component(G)\n",
    "        return len(L.nodes) / len(G.nodes)\n",
    "\n",
    "    def __component(self, G):\n",
    "        components = sorted(nx.strongly_connected_components(G), key=len)[::-1]\n",
    "        L = G.subgraph(components[0])\n",
    "        return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Execution\n",
    "\n",
    "Setting construct to true in the following block will have the script generate the .json files.\n",
    "However, as this takes a long itme, we have included the files from previous computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compute_json = False\n",
    "\n",
    "def construct():                   # recompute json and pollute climate\n",
    "    for G in tqdm(S.values()):\n",
    "        D = Metric(G)\n",
    "        D.build()\n",
    "        D.save()\n",
    "        \n",
    "def use_json():                    # use precomputed json and save-ish the planet\n",
    "    D = {}\n",
    "    dumps = [f\"../data/dumps/{target}\" for target in sorted(os.listdir('../data/dumps/'))]\n",
    "    for i in tqdm(range(len(dumps))):\n",
    "        with open(dumps[i], 'r') as data_file:\n",
    "            name = dumps[i].split('/')[-1].split('.')[0]\n",
    "            data = json.load(data_file)\n",
    "            D[i] = data\n",
    "    return D\n",
    "\n",
    "if compute_json:\n",
    "    construct()\n",
    "else:\n",
    "    D = use_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analysis\n",
    "\n",
    "We now proceede to show the following:\n",
    "1. Degree distribution\n",
    "2. Centrality measures\n",
    "3. Fitting plot\n",
    "4. Attack plot\n",
    "5. Virus spreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Degree distribution\n",
    "\n",
    "We plot the degree distribution of each graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "i = 0\n",
    "for ax in axes:\n",
    "    for ay in ax:\n",
    "        indeg = D[i]['degs']['in']\n",
    "        outdeg = D[i]['degs']['out']\n",
    "        ay.loglog(indeg[0], indeg[1], 'ro', label='Indegree')\n",
    "        ay.loglog(outdeg[0], outdeg[1], 'bo', label='Outdegree')\n",
    "        ay.legend()\n",
    "        i += 1\n",
    "\n",
    "plt.suptitle('Degree Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Blah blah blah blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Centrality Measuress\n",
    "\n",
    "At the time of crawling the network, two gnutella protocols were in fact in place (0.4 and 0.6)\n",
    "0.4 used query flooding and 0.6 used query routing. It is in unlcear to us what protocol\n",
    "was running on the nodes in our network. We have thus plotted the centrality\n",
    "measure that was relevant in either case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Betweenness centrality\n",
    "blah blah blach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "i = 0\n",
    "for ax in axes:\n",
    "    for ay in ax:\n",
    "        orignal = D[i]['centrality']['betweenness']\n",
    "        config = D[i]['C']['centrality']['betweenness']\n",
    "        ay.loglog(orignal[0], orignal[1], 'ro', label='Original Graph')\n",
    "        ay.loglog(config[0], config[1], 'bo', label='Configuration Model')\n",
    "        ay.legend()\n",
    "        i += 1\n",
    "\n",
    "plt.suptitle('Betweenness Centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness centrality\n",
    "To the extent that the network is using query flooding to find files, closeness centrality is essential.\n",
    "Therefor we have plotted it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "i = 0\n",
    "for ax in axes:\n",
    "    for ay in ax:\n",
    "        orignal = D[i]['centrality']['closeness']\n",
    "        config = D[i]['C']['centrality']['closeness']\n",
    "        ay.loglog(orignal[0], orignal[1], 'ro', label='Original Graph')\n",
    "        ay.loglog(config[0], config[1], 'bo', label='Configuration Model')\n",
    "        ay.legend()\n",
    "        i += 1\n",
    "\n",
    "plt.suptitle('Closeness Centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fitting\n",
    "\n",
    "We do some fitting yeah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "i = 0\n",
    "for ax in axes:\n",
    "    for ay in ax:\n",
    "        _, y = D['degs']['in']\n",
    "        y = np.array(y); mask = y >= 3\n",
    "        fit = powerlaw.Fit(y[mask], verbose=False)\n",
    "        fig = fit.plot_ccdf(label='CCDF', color='r', linestyle='--', marker='o')\n",
    "        fit.lognormal.plot_ccdf(ax=ay, color='c', linestyle='--', label='log-normal fit')\n",
    "        fit.power_law.plot_ccdf(ax=ay, color='g', linestyle='--', label='power-law fit')\n",
    "        fit.exponential.plot_ccdf(ax=ay, color='b', linestyle='--', label='exponential fit')\n",
    "        fit.truncated_power_law.plot_ccdf(ax=ay, color='k', linestyle='--', label='truncated power law fit')\n",
    "        plt.legend(); plt.title(f\"{self.name} fit\"); plt.ylabel(r\"$P(k>=x)$\"); plt.xlabel(r'$x$')\n",
    "        plt.savefig(f'../plots/{self.name}/fitting-{self.name}.png', dpi=300); plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Thoughts about the fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Robustness\n",
    "\n",
    "Cursory research on the Gnutella network describes it as inefficient and problematic\n",
    "Robustness is thus intersting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "i = 0\n",
    "for ax in axes:\n",
    "    for ay in ax:\n",
    "        kinds = ['random', 'degree', 'eigen', 'pagerank']; colors = 'brgy'\n",
    "        G = D[i]['attack']\n",
    "        #C = D[i]['C']['attack']\n",
    "        for i in range(len(G) - 1):\n",
    "            exec(f\"ay.plot(G[-1], G[i], colors[i], label='{kinds[i]}')\")\n",
    "            # exec(f\"plt.plot(C[-1], C[i], colors[i], linestyle='--', label='conf. {kinds[i]}')\")\n",
    "        #ay.xlabel('fraction attacked'); plt.ylabel('frac. of nodes in largest comp.')\n",
    "        ay.legend()\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
